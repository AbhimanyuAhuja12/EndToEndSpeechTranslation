{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59e1e01",
   "metadata": {},
   "source": [
    "# End to End Speech Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982ab584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f93e1",
   "metadata": {},
   "source": [
    "# Listed Parallel Audio Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ec8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_to_audio_files(folder_path):\n",
    "    audio_files = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.wav') and os.path.isfile(file_path):\n",
    "            audio_files.append(file_path)\n",
    "    return audio_files\n",
    "\n",
    "# Example usage\n",
    "source_folder_path = 'Source'  # Replace with the path to your source folder\n",
    "target_folder_path = 'target'  # Replace with the path to your target folder\n",
    "\n",
    "source_audio_files = folder_to_audio_files(source_folder_path)\n",
    "target_audio_files = folder_to_audio_files(target_folder_path)\n",
    "\n",
    "# Now source_audio_files and target_audio_files contain paths to the audio files in their respective folders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a92507",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "### \"\"\"Preprocesses audio data from WAV files for the model.\n",
    "\n",
    "###   Args:\n",
    "###     wav_files: A list of paths to WAV files (source and target).\n",
    "###       sr: Sampling rate (default: 16000 Hz).\n",
    "###     n_mels: Number of mel filterbanks (default: 128).\n",
    "###       seq_len: Maximum sequence length (default: 16).\n",
    "\n",
    "###   Returns:\n",
    "###       source_mels: A NumPy array of preprocessed mel spectrograms for source audio (shape: (num_samples, seq_len, n_mels)).\n",
    "###       target_mels: A NumPy array of preprocessed mel spectrograms for target audio (shape: (num_samples, seq_len, n_mels)).\n",
    "###   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad83d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(wav_files,sr=16000,n_mels=128,seq_len=16):\n",
    "\n",
    "  source_mels = []\n",
    "  target_mels = []\n",
    "\n",
    "  for filename in wav_files:\n",
    "    # Load audio and extract mel spectrograms\n",
    "    y,_= librosa.load(filename, sr=sr)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram).astype(np.float32)  # Normalize\n",
    "\n",
    "    # Pad or trim to fixed sequence length\n",
    "    if mel_spectrogram.shape[1] > seq_len:\n",
    "      mel_spectrogram = mel_spectrogram[:,:seq_len]  # Trim if longer\n",
    "    else:\n",
    "      pad_width = ((0, 0), (0, seq_len - mel_spectrogram.shape[1]))\n",
    "      mel_spectrogram = np.pad(mel_spectrogram,pad_width,mode='constant')\n",
    "\n",
    "    source_mels.append(mel_spectrogram)\n",
    "\n",
    "  source_mels = np.array(source_mels)\n",
    "  target_mels = source_mels  \n",
    "\n",
    "  return source_mels,target_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d821bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "source_mels, target_mels = preprocess_data( source_audio_files+target_audio_files )  # Combine for easier data splitting\n",
    "# Split data into training, validation, and (if needed) test sets (example using train-test split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_mels, val_mels, train_targets, val_targets = train_test_split(source_mels, target_mels, test_size=0.2, random_state=42)\n",
    "# Reshape for model input (add batch dimension)\n",
    "train_mels = train_mels.reshape((train_mels.shape[0], train_mels.shape[1], train_mels.shape[2], 1))\n",
    "val_mels = val_mels.reshape((val_mels.shape[0], val_mels.shape[1], val_mels.shape[2], 1))\n",
    "train_targets = train_targets.reshape((train_targets.shape[0], train_targets.shape[1], train_targets.shape[2], 1))\n",
    "val_targets = val_targets.reshape((val_targets.shape[0],val_targets.shape[1],val_targets.shape[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98574f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(keras.Model):\n",
    "    \"\"\"Encoder-decoder model architecture for audio transformation.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, latent_dim, rnn_units):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = keras.Sequential([\n",
    "            layers.TimeDistributed(layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same')),\n",
    "            layers.TimeDistributed(layers.MaxPooling1D(pool_size=2)),\n",
    "            layers.LSTM(units=rnn_units, return_sequences=True)\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = keras.Sequential([\n",
    "            layers.LSTM(units=rnn_units, return_sequences=True),\n",
    "            layers.TimeDistributed(layers.Dense(embedding_dim, activation='relu')),\n",
    "            layers.TimeDistributed(layers.Conv1D(filters=1, kernel_size=3, activation='linear', padding='same'))\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(128, 64, 256)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "history = model.fit(train_mels, train_targets, epochs=100, batch_size=32, validation_data=(val_mels, val_targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
